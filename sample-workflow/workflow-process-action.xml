<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.2" name="securityx-workflow-streaming-wf">
    <start to="hdfs-scan"/>
    
      <action name='hdfs-scan'>
         <java>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <configuration>
                <property>
                   <name>mapred.job.queue.name</name>
                   <value>${queueName}</value>
                 </property>
             </configuration>
             <main-class>com.hivedata.oozie.hdfsscanner.HDFSFileScanner</main-class>
             <arg>${hdfsconfig}</arg>
             <arg>${inputs}</arg>
             <arg>${numminutesback}</arg>
             <capture-output/>
         </java>
         <ok to="workflow-streaming-decision" />
         <error to="fail" />
      </action>


     <decision name="workflow-streaming-decision">
             <switch>
             	<case to="workflow-streaming-node">${(wf:actionData('hdfs-scan')['HASFILES']) == "YES"}</case>
             	<case to="end">${(wf:actionData('hdfs-scan')['HASFILES']) == "NO"}</case>
             	<default to="fail" />
             </switch>
      </decision>

      <action name="workflow-streaming-node">
        <map-reduce>
            <job-tracker>${jobTracker}</job-tracker>
            <name-node>${nameNode}</name-node>
            <prepare>
                <delete path="${workflowRoot}/output-data/streaming"/>
            </prepare>
            <streaming>
                <mapper>${mapCommand}</mapper>
                <reducer>/bin/cat</reducer>
            </streaming>
            <configuration>
                <property>
                    <name>mapreduce.reduce.class</name>
                    <value>org.apache.hadoop.mapred.lib.IdentityReducer</value>
                </property>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
                <property>
                    <name>mapred.input.dir</name>
                    <value>${wf:actionData('hdfs-scan')['INPUTFILES']}</value>
                </property>
                <property>
                    <name>mapred.output.dir</name>
                    <value>${outputs}</value>
                </property>
            </configuration>
            <file>${workflowRoot}/mapper.py#mapper.py</file>
        </map-reduce>
        <ok to="hive-add-partition"/>
        <error to="fail"/>
    </action>


    <end name="end"/>


</workflow-app>
